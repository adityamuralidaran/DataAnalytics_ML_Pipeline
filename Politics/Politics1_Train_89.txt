Facts hold a sacred place in Western liberal democracies. Whenever democracy seems to be going awry, when voters are manipulated or politicians are ducking questions, we turn to facts for salvation.
But they seem to be losing their ability to support consensus. PolitiFact has found that about 70 percent of Donald Trump’s “factual” statements actually fall into the categories of “mostly false,” “false” and “pants on fire” untruth.
For the Brexit referendum, Leave argued that European Union membership costs Britain 350 million pounds a week, but failed to account for the money received in return.
The sense is widespread: We have entered an age of post-truth politics.
As politics becomes more adversarial and dominated by television performances, the status of facts in public debate rises too high. We place expectations on statistics and expert testimony that strains them to breaking point. Rather than sit coolly outside the fray of political argument, facts are now one of the main rhetorical weapons within it.
How can we still be speaking of “facts” when they no longer provide us with a reality that we all agree on? The problem is that the experts and agencies involved in producing facts have multiplied, and many are now for hire. If you really want to find an expert willing to endorse a fact, and have sufficient money or political clout behind you, you probably can.
The combination of populist movements with social media is often held responsible for post-truth politics. Individuals have growing opportunities to shape their media consumption around their own opinions and prejudices, and populist leaders are ready to encourage them.
But to focus on recent, more egregious abuses of facts is to overlook the ways in which the authority of facts has been in decline for quite some time. Newspapers might provide resistance to the excesses of populist demagogy, but not to the broader crisis of facts.
The problem is the oversupply of facts in the 21st century: There are too many sources, too many methods, with varying levels of credibility, depending on who funded a given study and how the eye-catching number was selected.
According to the cultural historian Mary Poovey, the tendency to represent society in terms of facts first arose in late medieval times with the birth of accounting. What was new about merchant bookkeeping, Dr. Poovey argued, was that it presented a type of truth that could apparently stand alone, without requiring any interpretation or faith on the part of the person reading it.
In the centuries that followed, accounting was joined by statistics, economics, surveys and a range of other numerical methods. But even as these methods expanded, they tended to be the preserve of small, tight-knit institutions, academic societies and professional associations who could uphold standards. National statistical associations, for example, soon provided the know-how for official statistics offices, affiliated with and funded by governments.
In the 20th century, an industry for facts emerged. Market-research companies began to conduct surveys in the 1920s and extended into opinion polling in the 1930s. Think tanks like the American Enterprise Institute were established during and after World War II to apply statistics and economics to the design of new government policies, typically in the service of one political agenda or another. The idea of “evidence-based policy,” popular among liberal politicians in the late 1990s and early 2000s, saw economics being heavily leaned on to justify government programs, in an allegedly post-ideological age.
Of course the term “fact” isn’t reserved exclusively for numbers. But it does imply a type of knowledge that can be reliably parceled out in public, without constant need for verification or interpretation.
Yet there is one much more radical contributor to our post-truth politics that could ultimately be as transformative of our society as accounting proved to be 500 years ago.
We are in the middle of a transition from a society of facts to a society of data. During this interim, confusion abounds surrounding the exact status of knowledge and numbers in public life, exacerbating the sense that truth itself is being abandoned.
The place to start in understanding this transition is with the spread of “smart” technologies into everyday life, sometimes called the “internet of things.” Thanks to the presence of smartphones and smartcards in our pockets, the dramatic uptake of social media, the rise of e-commerce as a means of purchasing goods and services, and the spread of sensory devices across public spaces, we leave a vast quantity of data in our wake as we go about our daily activities.
Like statistics or other traditional facts, this data is quantitative in nature. What’s new is both its unprecedented volume (the “big” in big data) and also the fact that it is being constantly collected by default, rather than by deliberate expert design. Numbers are being generated much faster than we have any specific use for. But they can nevertheless be mined to get a sense of how people are behaving and what they are thinking.
The promise of facts is to settle arguments between warring perspectives and simplify the issues at stake. For instance, politicians might disagree over the right economic policy, but if they can agree that “the economy has grown by 2 percent” and “unemployment is 5 percent,” then there is at least a shared stable reality that they can argue over.
The promise of data, by contrast, is to sense shifts in public sentiment. By analyzing Twitter using algorithms, for example, it is possible to get virtually real-time updates on how a given politician is perceived. This is what’s known as “sentiment analysis.”
There are precedents for this, such as the “worm” that monitors live audience reaction during a televised presidential debate, rising and falling in response to each moment of a candidate’s rhetoric. Financial markets represent the sentiments of traders as they fluctuate throughout the day. Stock markets never produce a fact as to what Cisco is worth in the way that an accountant can; they provide a window into how thousands of people around the world are feeling about Cisco, from one minute to the next.
Journalists and politicians can no more ignore a constant audit of collective mood than C.E.O.s can ignore the fluctuations in their companies’ share prices. If the British government had spent more time trying to track public sentiment toward the European Union and less time repeating the facts of how the British economy benefited from membership in the union, it might have fought the Brexit referendum campaign differently and more successfully.
Dominic Cummings, one of the leading pro-Brexit campaigners, mocked what he called outdated polling techniques. He also asked one pollster to add a question on “enthusiasm,” and, employing scientists to mine very large, up-to-the-minute data sets, to gauge voter mood and to react accordingly with ads and voter-turnout volunteers.
It is possible to live in a world of data but no facts. Think of how we employ weather forecasts: We understand that it is not a fact that it will be 75 degrees on Thursday, and that figure will fluctuate all the time. Weather forecasting works in a similar way to sentiment analysis, bringing data from a wide range of sensory devices, and converting this into a constantly evolving narrative about the near future.
However, this produces some chilling possibilities for politics. Once numbers are viewed more as indicators of current sentiment, rather than as statements about reality, how are we to achieve any consensus on the nature of social, economic and environmental problems, never mind agree on the solutions?
Conspiracy theories prosper under such conditions. And while we will have far greater means of knowing how many people believe those theories, we will have far fewer means of persuading them to abandon them.
