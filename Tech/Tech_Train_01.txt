The queasy truth at the heart of Facebook’s Cambridge Analytica scandal, which is so far the company’s defining disgrace of 2018, is that its genesis became scandalous only in retrospect. The series of events that now implicate Facebook began in 2014, in plain view, with a listing on Amazon’s Mechanical Turk service, where users can complete small tasks for commensurately modest sums of cash. In exchange for installing a Facebook app and completing a survey — in the process granting the app access to parts of your Facebook profile — you would get around a dollar. Maybe two.
This was a great deal, at least by the standards of the time. Facebook users were then accustomed to granting apps permission to see their personal data in exchange for much less. It was the tail end of a Facebook era defined by connected apps: games like FarmVille, Candy Crush and Words With Friends; apps that broadcast your extra-Facebook activities, like Spotify and Pinterest; and apps that were almost explicitly about gathering as much useful data as possible from users, like TripAdvisor’s Cities I’ve Visited app, which let you share a digital pushpin map with your friends.
Most of these apps, when installed, demanded permission to access “your profile info,” which could include things like your activity, birthday, relationship status, interests, religious and political views, likes, education and work history. They could also collect information about users’ friends, multiplying their reach. In providing a marketplace for such apps, Facebook made it easy for users to extend their extraordinarily intimate relationship with the site to thousands of third-party developers. One of them turned out to be connected to Cambridge Analytica, which was using the data for right-wing political campaigns — a fact that was lucidly and widely reported as early as 2015 but promptly lost in the roiling insanity of primary season. (As of Facebook’s most recent admission, data was collected on as many as 87 million users.)
Not that more exposure in the news cycle would have mattered much back then. It was self-evidently absurd to grant a virtual-farming game access to your religious views, but that’s just how the platform worked at the time, and so we got used to it, much in the same way we got used to conducting our private lives on any other corporate platform. (When Gmail first started in 2004, the fact that it placed ads based on the contents of users’ emails was considered invasive. That feeling passed; Google continued scanning consumer email until 2017, and Gmail now has more than a billion users.) Still, these individually trivial decisions never gave us cause to confront just how much we had come to trust Facebook.
Whenever you sign up for any free service, you’re aware, in the loosest terms, that you’re giving up something. It usually includes a license to use the content you create — be it capital-C Content, like public posts, or things you would intuitively understand as more private, like your direct messages and which profiles you look at — however the service pleases. You could imagine the problem with all this, if asked: You’re giving up your privacy to a company that owes you nothing and could end up doing basically anything. But a user concerned about how ad-supported web services would infringe on his privacy is trapped in a sort of predictive hell: always right, but always feeling wrong; never quite able to say what will happen, but always expecting it; both constantly vindicated and yet feeling he’s crying wolf.
And its users unwittingly took part in a social-data boom and are only now starting to feel the consequences. The ease and speed with which internet users trust tech companies with the data they crave are wildly out of proportion with the risks they’re assuming in doing so, in no small part because, out of negligence or institutional blindness, the companies couldn’t anticipate the risks themselves. After all, Facebook spent years fortifying itself against hackers and spammers only to be rolled by political actors posing as app developers.
This pattern — a boom based on credible-seeming promises that are a little too good to be true, followed years later by a bust confirming that they indeed were — is familiar from a different world: finance. The financial crisis was materially devastating in a way that a privacy crisis couldn’t be; poor stewardship of user data isn’t going to evict people from their homes. But among the lingering and underrated consequences of the collapse of the global financial system was a total loss in trust in the institutions that were supposed to prevent it. The disorienting and thoroughly unsatisfying Cambridge Analytica saga is a preview of what trailing indicators of the collapse of the data boom might look like: revealing signs, evident years later, that something was rotten with these arrangements, arriving too late to be actionable but soon enough to foster resentment against companies and services on which we’ve come to depend.
Experiences that test our trust of the free-services-for-personal-data internet are accumulating and threaten to become more personal: the failure of Twitter to ban someone who harassed or threatened you; a small but embarrassing email hack resulting in a scammer asking old friends for money and concluding with an admonishment from your provider that you just needed a better password; an identity theft, a suffering credit score and then news of a hack at Equifax, a service to which you never even chose to provide data. Or it could be nothing more than an eerily well-targeted ad, one that suggests that a certain service — maybe one you never even meant to interact with — knows things about you that you don’t remember telling it.
The wider consequences of these arrangements are harder to quantify and sometimes even to see. They are: a social-media ecosystem that has annexed the news and the public sphere; nascent but increasingly assertive systems of identity and social currency that seek to transcend borders while answering only to investors; billions of lives’ worth of trustingly volunteered data in the hands of companies that might want to make money from it, or that might have no need for it anymore, or that might go out of business, change ownership or simply forget what they had in the first place. Perhaps someone — a new partner, an enterprising researcher, a repressive government — might, one day, discover new uses for the data.
A loss of faith in tech companies as semipublic infrastructure would also arrive simultaneously with an understanding that that’s what they had been all along: services that we depended on, ones we gave ourselves to, and that revealed themselves to be — or merely became — the sorts of services we’d rather not. They’re not too big to fail in the banking sense. But they’re similarly hard to budge, having constructed entire modes of interaction, consumption and identity verification that are now intimately interwoven with our lives, so all-encompassing that they’ve practically become invisible. To stop using these products is to leave the internet, and these companies made it their mission to make sure there isn’t anywhere else to go.
In a recent conference call with reporters, Mark Zuckerberg, the company’s chief executive, confessed that Facebook ought to do a better job of putting privacy tools in front of users, but in the same breath he encouraged people to use the tools and to “make sure that they’re comfortable with how their information is used on our services and others.” It was a gesture at dusty old user agreements — an emphasis on the fact that we technically consented to this, and that if he was expected to hold up his end of the deal, we should be too. It called to mind a line of the financial industry’s post-crisis defense: Borrowers should have read the fine print. But for the tech industry, this posture is even more of a stretch: It’s an argument for honoring the status quo, coming from the people who have made it their mission to destroy it.
