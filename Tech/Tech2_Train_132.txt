SEATTLE — Never mind Terminator-like killer robots. Artificial intelligence researchers are grappling with more realistic questions like whether their creations will take too many jobs from humans.
Eight years after leading artificial intelligence scientists said their field did not need to be regulated, the question of government oversight has re-emerged as the technology has rapidly progressed.
On Tuesday, at an event sponsored by the White House Office of Science and Technology Policy, legal specialists and technologists explored questions about autonomous systems that would increasingly make decisions without human input in areas like warfare, transportation and health.
Still, despite improvement in areas like machine vision and speech understanding, A.I. research is still far from matching the flexibility and learning capability of the human mind, researchers at the conference said.
“The A.I. community keeps climbing one mountain after another, and as it gets to the top of each mountain, it sees ahead still more mountains,” said Ed Felten, a computer scientist who is a deputy chief technology officer in the Office of Science and Technology Policy.
The half-day program, co-sponsored with the University of Washington School of Law and the university’s Tech Policy Lab, was the first of four events the White House has planned that will focus on law and policy, as well as the social and economic implications of autonomous machine research.
“These issues of A.I. and machines learning were popping up all over the government, and there was an opportunity to get more coordinated in how we address them,” Dr. Felten said.
After 25 artificial intelligence researchers met in 2009, the group, sponsored by the Association for the Advancement of Artificial Intelligence, reported that there was no imminent danger from a technology that had prompted fears of Hollywood-style weapons and advanced economies that were devoid of human workers.
Leading technologists and scientists have also pondered the possibility that artificial intelligence, like genetic engineering, might soon constitute an existential threat to the human race.
In recent years, the debate has spread to the broader social impact of autonomous programs that will perform tasks like driving cars and offering medical and financial advice.
One challenge for A.I., according to a number of the researchers who spoke, is that the public perception about the threat of A.I. has largely been shaped by Hollywood.
“Certainly, Hollywood has played a tremendous role with vision like Skynet,” the computer network that turns on humans in the “Terminator” movies, said Oren Etzioni, chief executive of the Allen Institute for Artificial Intelligence, a nonprofit research group funded by the Microsoft co-founder Paul Allen. “It’s pretty much always the case in science fiction that A.I. is this monolithic entity that is scheming to take over.”
He cautioned that attention-getting feats like Google’s AlphaGo program, which defeated a human champion in the board game Go, had plenty of humans behind the machine doing the work.
At the same time, despite the consensus about the limits of today’s technology, many of the panelists wrestled with new challenges presented by A.I. systems that were appearing in virtually all walks of life, including services delivered by smartphones and algorithms that guide missiles.
Kate Crawford, a principal researcher at Microsoft Research, called on the industry to add ethics to the professional training of engineers. “We need to start changing the skill set of the people who are going to be the data scientists of the future and the A.I. creators of the future,” she said.
A.I. systems are pervasive, Ms. Crawford said, pointing to a doll like Hello Barbie, which speaks and listens.
“You might think that’s a fantastic toy, that’s really wonderful,” she said. “What you don’t realize is that it is the front to this huge data ingestion machine that is taking all of those statements by that child and then using them for a whole range of purposes.”
